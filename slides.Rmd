---
title: Working with Geospatial Hydrologic Data Using Web Services
subtitle: Working with R
author: Mike Johnson^[Lynker, NOAA-Affiliate, fa]
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: TRUE
    highlight: tango
    theme: cerulean
---

```{r setup, include=FALSE}
oldoption <- options(scipen = 9999)
options(scipen = 9999)
knitr::opts_chunk$set(fig.width = 6, 
                      message = FALSE, 
                      warning = FALSE, 
                      comment = "", 
                      cache = FALSE, 
                      fig.retina = 3)


library(ggplot2)
library(patchwork)
```

```{r, echo=FALSE, eval = FALSE}
htmltools::img(src = knitr::image_uri('img/lynker.png'), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;')
```

# Introduction

This document carries us through some of the basics for finding and accessing data from the internet using R. While the details (e.g. specific functions) are R based, the concepts apply to all programming languages. 

> Section 1: R basics, and foundation spatial libraries

> Section 2: Key hydrology based packages (focus on data access)

> Section 3: Web Data without services 

> Section 4: Hands on example 

# Section 1: Spatial Data Foundations

## Basics for R

Install guides can be found [here](https://mikejohnson51.github.io/spds/assignment-01.html) for R and RStudio

### Getting R packages

 - "R packages are extensions to the base R statistical programming language. Packages contain "code, data, and documentation that can be installed by users of R" [[wikipedia](https://en.wikipedia.org/wiki/R_package)]
 
 - `install.packages(name)` can install packages from [CRAN](https://cran.r-project.org/)
 
 - `remotes::install_github("user/repo")` can install packages from  Github that are not released to CRAN (development versions, personal repos, ect.)
 
 - Packages are loaded (attached) into a working environment by calling `library(name)`
 
 - Once attached all objects (code, data, and documentation) of that package are exposed for use. 

### R function `syntax`

 - Functions are code snippets that can be reused, for common and/or complex tasks. Functions can be developed and shared in packages.
 
 - In R, `package::function` is the syntax for calling functions from a package that has not been attached, or, that has a conflicting function name. 
 
 -  _Example:_ `sf::read_sf()` calls the `read_sf` function from the `sf` package
 
 - Preceding a function with a `?` in brings up the help page for that function 
 
```{r}
?sf::read_sf
```
 
 - Hashtags (`#`) inserted before lines of code are comments, they are present for human use not machine interpetation
 
```{r}
# this is a comment, below is code
2 + 2
```

## Spatial Data

We are going to discuss spatial data for hydrology in two contexts. 

- "Vector" data are comprised of points, lines, and polygons that represent discrete spatial entities, such as a river, watershed, or stream gauge.

- "Raster" data divides spaces into rectilinear cells (pixels) to represent spatially continuous phenomena, such as elevation or the weather. The cell size (or resoltion) defines the fidelty of the data.

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-vec-raster.jpg")
```

### Communities, libraries and Standards 

- The spatial community is built on a few organizing communities (e.g. [OSgeo](https://www.osgeo.org) and [OGC](https://www.ogc.org)) _and_ a few core libraries that drive all spatial software (R, python, QGIS, ArcMap)!

These libraries include: 

  - **PROJ** --> projections, transformations  
  
  - **GEOS** --> Geometry operations (measures, relations)
  
  - **GDAL** --> geodata abstraction and processing (read, write)

### Simple Features Model

Simple Features (officially Simple Feature Access) is both an OGC and International Organization for Standardization (ISO) standard that specifies how (mostly) two-dimensional geometries can represent and describe objects in the real world. 

It describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.

It outlines how the spatial elements of POINTS (XY locations with a specific coordinate reference system) extend to LINES, POLYGONS and GEOMETRYCOLLECTION(s).

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-sf-model.png")
```

### sf: simple features

In R, the [`sf` package](https://cran.r-project.org/web/packages/sf/index.html) provides "_support for simple features, a standardized way to encode spatial vector data.... [and] Binds to 'GDAL' for reading and writing data, to 'GEOS' for geometrical operations, and to 'PROJ' for projection conversions and datum transformations._" 

Therefore when using R, you are using an interface to the core community standards, softwares, and practices (this is no exclusive to R). TO highlight this we can install (do this once) and attach `sf` to view the external dependencies versions of the libraries linked to `sf`. 

```{r}
# install.packages("sf")
library(sf)

sf_extSoftVersion()
```

The bindings to these lower-level C libraries, and, the larger `sf` ecosystem in R can be seen below:

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/09-sf-depends.png")
```

What is novel about the `sf` implementation in the R ecosystem is the way in which the authors built on the list structure in R to store simple feature geometries (sfg) as part of a larger data.frame using a simple feature geometry list-column (sfg). The collection of attribute and spatial information define a simple feature that can be operated on in both table (SQL) and spatial (GEOS, ect) contexts. Not only does this allow us to make the most use of the growing spatial community but _also_ of the growing data science community (see `ggplot`, `dplyr`, `data.table`, `dbplyr`, `arrow`, ect.)

In practice, an `sf` object in R looks like the following:

```{r, fig.align='center', echo = FALSE, out.width="75%" }
knitr::include_graphics("img/sf_xfig.png")
```

This extends the idea of ["tidy" data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) in that each row represents one observation, which has one geometric representation of the real world feature it describes.

#### Basic use:

```{r}
# Define file path
(filename <- system.file("shape/nc.shp", package="sf"))

# read in file
(nc <- read_sf(filename) )
```

```{r}
# Map
plot(nc['SID79'])

# Spatial measures!
head(st_area(nc))

# Spatial operations!
{
  st_union(nc) |> plot()
  
  st_centroid(nc)$geometry |> plot(col = "red", add = TRUE)
}

# data science operations
library(dplyr)

{
 plot(nc$geometry)
 plot(slice_max(nc, AREA, n = 10)$geometry, 
      col = "red", add = TRUE)
 plot(slice_max(nc, AREA, n =5)$geometry, 
      col = "yellow", add = TRUE)
 plot(slice_max(nc, AREA, n = 1)$geometry, 
      col = "green", add = TRUE)
}

```

### terra: 

- While `sf` is a leading package for working with vector data, 
[`terra`](https://rspatial.github.io/terra/reference/terra-package.html) is a primary R package for working with raster data. `terra` also operates on vector data but we focus on `sf` for those purposes

- `terra` builds on the older `raster` package and provides methods for low-level data manipulation as well as high-level global, local, zonal, and focal computations. The predict and interpolate methods facilitate the use of regression type (interpolation, machine learning) models for spatial prediction, including with satellite remote sensing data. Processing of very large files is supported. 

Like `sf`, `terra` links to GDAL, and the version and drivers can be viewed using package support functions:

```{r}
# install.packages(terra)
library(terra)

gdal()

DT::datatable(gdal(drivers = TRUE))
```

#### Basic Use

```{r}

(filename <- system.file("ex/meuse.tif", package="terra"))

(r <- rast(filename))

plot(r, main='SpatRaster')
```

### mapview: 

[mapview](https://cran.r-project.org/web/packages/mapview/index.html) is a `leaflet` helper package designed to quickly and conveniently create interactive visualizations of spatial data. 

#### Basic Useage:

```{r}
# install.packages(mapview)
library(mapview)

mapview(nc)
```

Together, `sf`/`terra`/`mapview` provide much of the functionality needed to get the best of both a programmatic and GUI driven GIS!

# Section 2: Geospatial/Hydrology focused packages:

With a strong grasp on the basic spatial libraries for reading, writing, and manipulating spatial data, we can move onto packages that seek to support the hydrologic and earth systems communities.

## AOI

AOI is _NOT_ on CRAN but provides interfaces for geocoding and retrieving spatial boundaries (e.g state, county, and country). When working with web-based,  subsetting services, the ability to quickly define common features is really convenient.

### Basic Useage:

```{r}
# remotes::install_github("mikejohnson51/AOI")
library(AOI)

# state
aoi_get(state = "CO") |> mapview()

# county
aoi_get(state = "AL", county = "Tuscaloosa") |> mapview()

# country
aoi_get(country = "Ukraine") |> mapview()

# geocoding
geocode("NOAA National Water Center", pt = TRUE) |> mapview()
```

## nhdplusTools

[nhdplusTools](https://usgs-r.github.io/nhdplusTools/index.html) is a R package for working with the NHD and the NHD data model. The package is robust and covers 5 primary use topics that include:

1. [data access](https://usgs-r.github.io/nhdplusTools/reference/index.html#data-access)
2. [data discovery and subsetting (via web services)](https://usgs-r.github.io/nhdplusTools/reference/index.html#discovery-and-subsetting)
3. [Indexing and Network Navigation](https://usgs-r.github.io/nhdplusTools/reference/index.html#indexing-and-network-navigation)
4. [Network Navigation](https://usgs-r.github.io/nhdplusTools/reference/index.html#network-navigation)
5. [Network Attributes](https://usgs-r.github.io/nhdplusTools/reference/index.html#network-attributes)

Given the focus here on geospatial data via web services, we will look at functions supporting use case 2.

```{r}
# install.packages('nhdplusTools')
library(nhdplusTools)

grep("get_", ls("package:nhdplusTools"), value = TRUE)
```

### Basic Use

Here we are interested in getting data for an AOI around Boston. We use `AOI::aoi_get` to get the OpenStreetMap representation of Boston, and initialize an empty list to populate: 

```{r}
boston     = list()
boston$AOI = aoi_get("Boston")
```

```{r}
boston$nhdplus      <- get_nhdplus(AOI = boston$AOI) 
boston$waterbodies  <- get_waterbodies(AOI = boston$AOI) 
boston$gages        <- get_nwis(AOI = boston$AOI) 
boston$huc12        <- get_huc12(AOI = boston$AOI) 

# How many features per entry?
sapply(boston, nrow)
```

```{r}
mapview(boston)
```

In both the NHDPlus data model, and the emerging international OGC standard for representing surface water features [HY_features](https://docs.opengeospatial.org/is/14-111r6/14-111r6.html) it's recognized that hydrologic entities can be "realized" in a number of ways. 

For example the holistic notion of a 'catchment' can be realized as a 'divide', 'flowpath', 'outlet', or other representation. The `nhdplusTools::get_nhdplus` functions supports getting 3 realizations of the NHD features that are consistent with the NHDPlus Data model (`outlet`, `flowline`, `catchment`, and `all`).

```{r}
boulder <- get_nhdplus(aoi_get("boulder"), realization = "all")
mapview(boulder)
```

## dataRetrival

The USGS supported [dataRetrival](https://usgs-r.github.io/dataRetrieval/) package provides retrieval functions for USGS and EPA hydrologic and water quality data. Recently a Network Linked Data Index (NLDI) client was also added to the [package](http://usgs-r.github.io/dataRetrieval/articles/nldi.html).

### Basic Use: Streamflow Data

```{r}
#install.package('dataRetrieval')
library(dataRetrieval)

# Supply gage ids from Boston example
# Parameter codes can be found with (`dataRetrieval::parameterCdFile`)
# "00060" is streamflow

flows = readNWISdv(site = boston$gages$site_no, parameterCd = "00060") |> 
  renameNWISColumns()

ggplot(data = flows) + 
  geom_line(aes(x = Date, y = Flow)) + 
  facet_wrap('site_no')
```

### NLDI client

The hydro [Network-Linked Data Index (NLDI)](https://labs.waterdata.usgs.gov/about-nldi/index.html) is a system that can index spatial and river network-linked data and navigate the river network to allow discovery of indexed information.

The NLDI service requires the following:

1. A starting entity (`comid`, `nwis`, `wqp`, `location` and more). 
 - The "more" includes the ever-growing set of resources available in the NLDI service that can be accessed with the `get_nldi_sources` function.

```{r}
DT::datatable(get_nldi_sources())
```

2. A direction to navigate along the network
  - UM: upper mainstem
  - UT: upper tributary
  - DM: downstream mainstem
  - DD: downstream divergence

3. Features to find along the way!
  - Any of the above features (e.g. `get_nldi_sources`)
  - flowlines
  - basin

### Basic use: NLDI

The NLDI client in `dataRetrival` operates by speficing the origin, the navigation mode, and the features to find. In the following example, we want to navigate along the upstream tributary of COMID 101 (up to 1000 km) and find all flowline features, nwis gages, and the contributing drainage area. 

```{r}
nldi = findNLDI(comid       = 101, 
             nav         = "UT", 
             find        = c("flowline", "nwis", 'basin'),
             distance_km = 1000)

sapply(nldi, nrow)
```

```{r}
mapview(nldi)
```

## nwmTools

While dataRetrival provides retrival functions for observed data, there is an increasing amount of simulated data available for use. Once example is the 3 retrospective simulations of the [National Water Model](https://water.noaa.gov/about/nwm). This data certainly falls under the umbrella of ["big data"](https://www.cuahsi.org/about/news/big-data-dreaming-a-42-year-conus-hydrologic-retrospective) and can be difficult to work with given each hourly file (for up to 42 years!) is stored in seperate, cloud based files.

[`nwmTools`](https://mikejohnson51.github.io/nwmTools/) provides retrieval functions for these retrospective streamflow data from the NOAA National Water Model reanalysis products:

### Basic Use: Retrival

```{r}
# remotes::install_github("mikejohnson51/nwmTools")
library(nwmTools)

nwis = readNWISdv(site = gsub("USGS-", "", nldi$UT_nwissite$identifier[10]), 
                  param = "00060") |> 
  renameNWISColumns()

nwm = readNWMdata(comid = nldi$UT_nwissite$comid[10])
```

### Basic Use: Aggregation

The timestep of the retrospective NWM data is hourly. In many cases, its more desirable to have the time step aggregated to a different temporal resolution. nwmTools provides a family of aggregation functions that allow you to specify the aggregation time period, and the aggregation summary statistic

```{r}
grep('aggregate_', ls("package:nwmTools"), value = TRUE)
```

```{r}
# Core function
nwm_ymd  = aggregate_ymd(nwm, "mean")
# User defined function
seasonal = aggregate_s(nwm, fun = function(x){quantile(x,.75)})
# Multiple functions
month    = aggregate_m(nwm, c('mean', "max"))

ggplot(data = nwis) + 
  geom_line(aes(x = Date, y = Flow * 0.028316846592)) + 
  labs(title = "NWIS Data") +
ggplot(data = nwm_ymd) + 
  geom_line(aes(x = ymd, y = flow_cms_v2.1)) + 
  labs(title = "NWM Daily Average") + 
ggplot(data = seasonal) + 
  geom_col(aes(x = season, y = flow_cms_v2.1)) + 
  labs(title = "NWM Seasonal 75%") +
ggplot(data = month) + 
  geom_col(aes(x = as.factor(month), y = flow_cms_v2.1_max)) +
  geom_col(aes(x = as.factor(month), y = flow_cms_v2.1_mean), fill = "red") +
  labs(title = "NWM Monthly Mean/Maximum")
```

## elevatr

elevatr is R package focused of returning elevation data as a SpatialPointsDataFrame from point elevation services or as a raster object from raster elevation services. Currently, the package supports access to the [Amazon Web Services Terrain Tiles](https://registry.opendata.aws/terrain-tiles/), the [Open Topography Global Datasets API](https://opentopography.org/developers/), and the [USGS Elevation Point Query Service](https://nationalmap.gov/epqs/).

### Basic Use
```{r}
# install.packages("elevatr")
library(elevatr)

x <- get_elev_raster(nldi$basin, z = 12)

plot(rast(x))
```

## opendap.catalog

One of the biggest challenges with Earth System and spatial research is extracting data. These challenges include not only finding the source data but then downloading, managing, and extracting the partitions critical for a given task.

Services exist to make data more readily available over the web but introduce new challenges of identifying subsets, working across a wide array of standards (e.g. non-standards), all without alleviating the challenge of finding resources.

In light of this, [opendap.catalog](https://mikejohnson51.github.io/opendap.catalog/) provides three primary services.


### 1. Generalized space (XY) and Time (T) subsets for remote and local NetCDF data with dap()
 - Handles data streaming, projection, and cropping. Soon to come is masking capabilities.

```{r}
library(opendap.catalog)

AOI = aoi_get(state = "FL")

dap <- dap(URL = "https://cida.usgs.gov/thredds/dodsC/bcsd_obs", 
           AOI = AOI, 
           startDate = "1995-01-01")

str(dap, max.level = 1)

plot(rast(dap))

###

lai = dap('/Users/mjohnson/mosaic_lai_glass_1.tif',
    AOI = AOI) 

{
  plot(lai)
  plot(st_transform(AOI$geometry, crs(lai)), add = TRUE)
}

```

### 2. A catalog of `r nrow(opendap.catalog::params)` web resources (as of 06/2022)

```{r}
glimpse(opendap.catalog::params)
```

With 14,160 web resources documented, there are simply too many resources to search through by hand unless you know exactly what you want. This voids the possibility of serendipitous discovery. So, we have added a generally fuzzy search tool to help discover datasets.

Say you want to find what resoruces there are for daily rainfall? `search` and `search_summary` can help:

```{r}
search("daily precipitation") |> 
  search_summary() |> 
  DT::datatable()

(s = search('gridmet daily precipitation'))
```

### (3) The ability to pass catalog elements to the generalized toolsets:

Now that you know the precipitation product you want, lets say you want to study the rainfall seen during Hurricane Harvey over Texas and Louisiana:

```{r}
(harvey = dap(catalog = s, 
             AOI = aoi_get(state = c("TX", "LA")), 
             startDate = "2017-08-17",
             endDate   = "2017-09-03"))

plot(harvey[[1]])

# ---- 
harvey_counties = aoi_get(state = c("TX", "LA"), county = "all")

{
  plot(sum(harvey[[1]]) * 0.001, 
       main = "Meters of rainfall: Hurricane Harvey")
  plot(harvey_counties$geometry, add = TRUE)
}
```

### (4) Tiled data streams

One of the key features of `dap` is the capacity to make requests over tiled resources. Some resources (like MODIS) and tiled in space, and some (like MACA, and LOCA) are tiled in time. Below we see an example of how a single `dap` call can consolidate a request that covers multiple spatial tiles:

```{r}
dap = dap(
    catalog = search("MOD16A2.006 PET"),
    AOI = AOI::aoi_get(state = "FL"),
    startDate = "2010-01-01",
    endDate   = "2010-01-31"
  )

plot(dap)
```

## zonal

Often getting spatial data is not the end goal and in many cases producing area based summarizations is critical.

[`zonal`](https://github.com/NOAA-OWP/zonal) is a package that wraps the excellent [`exactextractr`](https://isciences.gitlab.io/exactextractr/) package with some added performance and convenience for these types of tasks. To use zonal, you must supply a raster dataset (SpatRaster), a geometry set to summarize over, and a summary functions (similar to `nwmTools::aggregate_*`). Lastly, you must provide the column with a unique identifier for each polygon object, and whether the summary data should be joined (`join  = TRUE`) to the original summary data.

```{r}
#remotes::install_github("NOAA-OWP/zonal")
library(zonal)

# Summary options
zonal::ee_functions()
zonal::weight_functions()
```

For big data processing `zonal` offers a unique ability to produce and work with weight grids. However this is out of scope for this workshop:

### Basic Usage:

Returning to the Hurricane Harvey example, imagine you wanted the county wide total rainfall for each day:

```{r}
library(zonal)

system.time({
summary_data = execute_zonal(harvey, 
                             geom = harvey_counties, 
                             fun = "sum", 
                             ID = "fip_code", 
                             join = TRUE)
})

plot(summary_data[grepl("sum", names(summary_data))], border = NA,
     max.plot = 16)
```

# Section 3: Web data without a "service"

Often the needed data is not delivered by a service or function. This doesnot mean we are out of luck, but rather that we have to do a little of the "heavy lifting" ourselves. This final section highlights how to read tabluar, vector and raster data from URL using the http/https and s3 protocols.

These approaches are advantageous when you dont want to download data locally for analysis.

## Access tables

Many CSV readers in R have the ability to read from a URL. If your desired data is stored in CSV you have a number of options:

 - `base::read.csv`
 - `readr::read_csv`
 - `data.table::fread`

One example dataset in which there is not webservice client is the USACE National Inventory of Dams (NID) data. This data is stored as a CSV file.

## Basic usage:

```{r}
library(readr)

dams = read_csv('https://nid.usace.army.mil/api/nation/csv', 
                skip = 1) 

glimpse(dams)

```

## Access Raster Data

GDAL provides a number of [Virtual File System](https://gdal.org/user/virtual_file_systems.html) drivers to read compressed and network data. 

Three of the most useful for common spatial data tasks include:

- **vsizip** --> file handler that allows reading ZIP archives on-the-fly without decompressing them beforehand.

- **vsicurl** --> A generic file system handler exists for online resources that
  do not require particular signed authentication schemes

- **vsis3** --> specialized into sub-filesystems for commercial cloud storage services (e.g. AWS, also see  /vsigs/, /vsiaz/, /vsioss/ or /vsiswift/).

## Basic Usgage

Duke store [VRT](https://gdal.org/drivers/raster/vrt.html) files for the POLARIS soils dataset. `terra` can generate pointers to these datasets however data is not extracted until an operation is called on the data. As seen above `dap` provides simplified access to extract subsets of data for OpenDap and VRT endpoints.

```{r}

rast('/vsicurl/http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/vrt/alpha_mean_0_5.vrt') 

dap('/vsicurl/http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/vrt/alpha_mean_0_5.vrt', AOI = AOI::aoi_get(state = "CO", county = "Larimer")) |> 
  plot()
```

## Transect example

Here is an example where a global elevation dataset can quickly be used in conjuction with the NLDI client to plot the elevation transect of a the main river channel drainign to the USGS office in Fort Collins, CO!

```{r}

d = findNLDI(loc = geocode("2150 Centre Ave, Fort Collins, CO 80526", pt = TRUE),
             nav = "UM", 
             find = c("flowlines", "basin")) 

mapview(d)

COP30 = dap("/vsicurl/https://opentopography.s3.sdsc.edu/raster/COP30/COP30_hh.vrt", 
            AOI = d$basin)

transect = extract(COP30, vect(st_union(d$UM_flowlines))) |> 
  mutate(location = c(n():1))

ggplot(transect, aes(x = location, y = COP30_hh)) +
    geom_line() +
    geom_smooth()
```


## Vector 

## Basic Usgage: TIGER lines 
Since the base data reader of sf is GDAL the vsi capabilites also apply to vector data! The added challenge typically is that vecotor data (espesially shapefile data) is generally stored in zip files. This adds an extra layer of complexity when remotely reading these datasets. 

In this example we look at the US Census Bureus FTP server for TIGER road lines. The data is stored by by 5 digit (county) FIP code in zip files. To access this data we need to identify the FIP code of interst and then chain a `vsizip` with a `vsicurl` call.

```{r}
AOI  = aoi_get(state = "AL", county = "Tuscaloosa")
file = paste0('tl_2021_',AOI$fip_code, '_roads')
url  = paste0('/vsizip/{/vsicurl/https://www2.census.gov/geo/tiger/TIGER2021/ROADS/', file, '.zip}/',file,'.shp')

system.time({
  roads  = read_sf(url)
})

mapview(roads)
```

For those curious about the generalization of this, we can see that any GDAL backed software (here terra) can utilize the same url:

```{r}
system.time({
  roads2 = vect(url)
})

plot(roads2)
```

## Basic Usgage: Geoconnex JSON-LD:

Lastly, no IoW workshop would not be complete without highlighting the utility of the ever growing geoconnex system. This system build and maintains the infrastructure and data needed to "make water data as easily discoverable, accessible, and usable as possible."

One of the datasets that this system has minted Persistent Identifiers (PIDs) for is a collection of [USA mainstems](https://pubs.er.usgs.gov/publication/70216698). To be a "PID" a resource - in this case spatial feature - has to have a unique URI that can be resolved. This URI are store as JSON-LD, which, can be read by GDAL! So, if you can identify the geoconnex resource you want, you can read it into R and have confidence the URI will not change on you:


```{r}
feat = read_sf('https://geoconnex.us/ref/mainstems/2238605') 
mapview(feat$geometry)
```

